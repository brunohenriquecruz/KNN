{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício de classificação usando KNN\n",
    "\n",
    "Este exercício aborda a classificação de um conjunto de resultados usando o classificador KNN, um dos algoritmos de Machine Learning mais conhecidos e aplicados.\n",
    "\n",
    "O algoritmo do KNN é bem simpes. Ele pode ser sumarizado da seguinte forma:\n",
    "\n",
    "1. Calcular a distância entre a amostra desconhecida e todos os elementos do conjunto de classificação.\n",
    "2. Usamos uma função de dissimilaridade, como a distância Euclidiana, a distância de Manhattan, a Hamming, entre outras.\n",
    "3. O modelo então seleciona os K elementos mais próximos da amostra desconhecida.\n",
    "4. A classe é decidida por votação majoritária, ou seja, a amostra é classificada com a classe que obteve o maior número de vizinhos próximos.\n",
    "\n",
    "Para a construção do modelo, devemos usar alguns procedimentos como validação cruzada (cross validation), para garantir que este tem boa capacidade de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregando as bibliotecas\n",
    "\n",
    "Comece carregando no ambiente as bibliotecas que utilizaremos para completar o exercício. Vamos utilizar o numpy, o pandas e o matplotlib para carregar os valores, fazer a análise dos dados e a exibicição dos resultados, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos carregar um conjunto de dados para classificarmos. No pacote SciKit Learning, encontramos algumas bases de dados prontas para uso, como a Iris, que vimos anteriormente. Para este exercício, usaremos uma base diferente, a base de dados de exames de câncer de mama do hospital-escola de Winsconsin (mais informações sobre a base [aqui](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/).\n",
    "\n",
    "Carregue a função load_breast_cancer do pacote sklearn.datasets. Crie uma variável com o nome *universe* e carregue nela a base de dados. Em seguida, exiba os nomes dos atributos e as 5 primeiras linhas do conjunto de dados. Imprima também os nomes das classes. A primeira estará rotulada como 0 e a segunda como 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "universe = load_breast_cancer()\n",
    "atrib_names = universe['target_names']\n",
    "lab_data = universe.data\n",
    "class_name = universe['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Dados\n",
    "print(atrib_names)\n",
    "print(lab_data[:5])\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Treinando o modelo\n",
    "\n",
    "Agora, vamos treinar o modelo, usando o utilitário de validação cruzada do pacote Scikit Learning.\n",
    "\n",
    "Separe o conjunto de dados em 4 variáveis: X_treino, X_teste, Y_treino e Y_teste. Os arquivos X contém os dados de treino e teste e os conjuntos Y contém os rótulos correspondentes. Use a função train_test_split do pacote sklearn.model_selection. Não se esqueça de importá-la. Imprima os shapes do conjunto de dados e de rótulos de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(114, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(lab_data, class_name, test_size=0.2, random_state=42)\n",
    "print(X_treino.shape)\n",
    "print(X_teste.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que os conjuntos de dados foram separados, vamos instanciar o classificador e treiná-lo. Importe a classe KNeighborsClassifier do pacote sklearn.neighbors. Rode várias instâncias de treinamento e previsão, usando os conjuntos de treino e teste, variando o valor de K entre 1 e 25. Use o pacote sklearn.metrics para medir a acurácia de cada instância, salvando o resultado. Use o matplotlib para exibir a acurácia de cada instância. Não se esqueça de identificar os rótulos dos eixos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "neigh =  KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_treino, Y_treino)\n",
    "\n",
    "print(neigh)\n",
    "\n",
    "pred = neigh.predict(X_teste)\n",
    "print (accuracy_score(Y_teste, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9298245614035088\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9298245614035088\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9298245614035088\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9385964912280702\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.956140350877193\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.956140350877193\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=8, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.956140350877193\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.956140350877193\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9736842105263158\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9824561403508771\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=12, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9824561403508771\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9736842105263158\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=14, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9736842105263158\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=17, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=18, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=21, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=22, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=23, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.956140350877193\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=24, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "\n",
      "0.9649122807017544\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range (1,25):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_treino, Y_treino)\n",
    "    print (neigh)\n",
    "    print('\\n')\n",
    "    pred = neigh.predict(X_teste)\n",
    "    print (accuracy_score(Y_teste, pred))\n",
    "    print('\\n')\n",
    "    \n",
    "    #plt.pyplot(accuracy_score(Y_teste, pred))\n",
    "    #plt.xlabel('Number of Neighbors K')\n",
    "    #plt.ylabel('Y')\n",
    "    #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinando o modelo final e realizando previsões\n",
    "\n",
    "Observe pelo gráfico qual o menor valor de K que nos dá a maior acurácia. Treine um novo classificador usando o valor observado para K. Faça a previsão usando o conjunto de teste previamente separado e exiba a matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, faça uma análise de como a acurácia o classificador é afetado mudando-se a função de distância. Consulte a documentação da classe DistanceMetric do sklearn para verificar quais funções estão disponíveis (lembrando que o espaço vetorial deste conjunto de dados é real). Exiba um gráfico com as acurácias de cada função de distância, usando o melhor valor de K observado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
